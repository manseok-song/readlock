"""ì „ì‚¬ íŒŒì´í”„ë¼ì¸ - ì „ì²´ ì›Œí¬í”Œë¡œìš° ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ (Phase 2 í•˜ì´ë¸Œë¦¬ë“œ)"""

import asyncio
import shutil
import tempfile
from dataclasses import dataclass, field
from enum import Enum
from pathlib import Path
from typing import Callable, Optional

from src.audio.extractor import AudioExtractor, ExtractionConfig
from src.audio.analyzer import AudioAnalyzer, AudioMetadata
from src.stt.base import STTEngine, TranscriptionResult
from src.stt.gemini import GeminiSTT, GeminiConfig
from src.output.srt import SRTGenerator, SRTConfig, VTTGenerator
from src.output.json_export import JSONExporter, JSONExportConfig


class OutputFormat(str, Enum):
    """ì¶œë ¥ í¬ë§·"""
    SRT = "srt"
    VTT = "vtt"
    JSON = "json"
    TXT = "txt"


class PipelineMode(str, Enum):
    """íŒŒì´í”„ë¼ì¸ ëª¨ë“œ"""
    FAST = "fast"           # Geminië§Œ ì‚¬ìš© (ë¹ ë¥´ì§€ë§Œ íƒ€ì„ìŠ¤íƒ¬í”„ ë¶€ì •í™•)
    ACCURATE = "accurate"   # WhisperXë§Œ ì‚¬ìš© (ì •í™•í•˜ì§€ë§Œ GPU í•„ìš”)
    HYBRID = "hybrid"       # Gemini + WhisperX ì •ë ¬ (ê¶Œì¥)
    FULL = "full"          # í•˜ì´ë¸Œë¦¬ë“œ + LLM êµì • (ìµœê³  í’ˆì§ˆ)


@dataclass
class PipelineConfig:
    """íŒŒì´í”„ë¼ì¸ ì„¤ì •"""
    # ì˜¤ë””ì˜¤ ì„¤ì •
    audio_format: str = "wav"
    audio_sample_rate: int = 16000
    audio_channels: int = 1

    # íŒŒì´í”„ë¼ì¸ ëª¨ë“œ
    mode: PipelineMode = PipelineMode.FAST

    # STT ì„¤ì •
    stt_engine: str = "gemini"  # "gemini" ë˜ëŠ” "whisperx"
    gemini_model: str = "gemini-3-flash-preview"  # ê¸°ë³¸: Gemini 3 Flash (ë°±ì—…: gemini-3-pro)
    language: str = "ko"
    num_speakers: Optional[int] = None
    proper_nouns: Optional[list[str]] = None  # í›„ë³´ì ì´ë¦„/ì •ì±…ëª… íŒíŠ¸
    use_video_mode: bool = False  # ì˜ìƒ ëª¨ë“œ (ìë™ ê°ì§€)
    remove_fillers: bool = False  # í•„ëŸ¬ ì œê±° (ë¹„í™œì„±í™”)
    election_debate_mode: bool = True  # ì„ ê±° í† ë¡ íšŒ ëª¨ë“œ (ê¸°ë³¸ í™œì„±í™”)

    # Phase 2 ì˜µì…˜
    enable_timestamp_alignment: bool = False  # WhisperX íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬
    enable_llm_correction: bool = False       # Claude LLM êµì •
    enable_pyannote_diarization: bool = False # Pyannote í™”ìë¶„ë¦¬

    # ì¶œë ¥ ì„¤ì •
    output_formats: list[OutputFormat] = field(
        default_factory=lambda: [OutputFormat.SRT]
    )
    include_speaker_labels: bool = True

    # ê¸°íƒ€
    temp_dir: Optional[str] = None
    cleanup_temp: bool = True
    device: str = "cuda"  # WhisperXìš©
    vram_gb: Optional[int] = None  # GPU VRAM (GB) - ìë™ ëª¨ë¸ ì„ íƒìš©


@dataclass
class PipelineResult:
    """íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ê²°ê³¼"""
    success: bool
    transcription: Optional[TranscriptionResult] = None
    output_files: dict[str, Path] = field(default_factory=dict)
    audio_metadata: Optional[AudioMetadata] = None
    error: Optional[str] = None
    processing_info: dict = field(default_factory=dict)


class TranscriptionPipeline:
    """ì „ì‚¬ íŒŒì´í”„ë¼ì¸ (Phase 2 í•˜ì´ë¸Œë¦¬ë“œ ì§€ì›)"""

    def __init__(
        self,
        config: Optional[PipelineConfig] = None,
        progress_callback: Optional[Callable[[str, float], None]] = None
    ):
        self.config = config or PipelineConfig()
        self.progress_callback = progress_callback

        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self._extractor: Optional[AudioExtractor] = None
        self._analyzer: Optional[AudioAnalyzer] = None
        self._stt_engine: Optional[STTEngine] = None
        self._timestamp_aligner = None
        self._llm_corrector = None
        self._speaker_diarizer = None
        self._temp_dir: Optional[Path] = None

        # ëª¨ë“œì— ë”°ë¥¸ ì„¤ì • ìë™ ì ìš©
        self._apply_mode_settings()

    def _apply_mode_settings(self):
        """ëª¨ë“œì— ë”°ë¥¸ ì„¤ì • ìë™ ì ìš©"""
        if self.config.mode == PipelineMode.FAST:
            self.config.stt_engine = "gemini"
            self.config.enable_timestamp_alignment = False
            self.config.enable_llm_correction = False

        elif self.config.mode == PipelineMode.ACCURATE:
            self.config.stt_engine = "whisperx"
            self.config.enable_timestamp_alignment = False
            self.config.enable_llm_correction = False

        elif self.config.mode == PipelineMode.HYBRID:
            self.config.stt_engine = "gemini"
            self.config.enable_timestamp_alignment = True
            self.config.enable_llm_correction = False

        elif self.config.mode == PipelineMode.FULL:
            self.config.stt_engine = "gemini"
            self.config.enable_timestamp_alignment = True
            self.config.enable_llm_correction = True

    def _report_progress(self, stage: str, progress: float) -> None:
        if self.progress_callback:
            self.progress_callback(stage, progress)

    def _get_extractor(self) -> AudioExtractor:
        if self._extractor is None:
            self._extractor = AudioExtractor(ExtractionConfig(
                format=self.config.audio_format,
                sample_rate=self.config.audio_sample_rate,
                channels=self.config.audio_channels
            ))
        return self._extractor

    def _get_analyzer(self) -> AudioAnalyzer:
        if self._analyzer is None:
            self._analyzer = AudioAnalyzer()
        return self._analyzer

    def _get_stt_engine(self) -> STTEngine:
        if self._stt_engine is None:
            if self.config.stt_engine == "gemini":
                # ëª¨ë¸ë³„ ì„¤ì •
                gemini_config = GeminiConfig(model=self.config.gemini_model)

                # Gemini 3 FlashëŠ” thinking_levelê³¼ temperature ì„¤ì • í•„ìš”
                if "gemini-3" in self.config.gemini_model:
                    gemini_config.thinking_level = "medium"
                    gemini_config.media_resolution = "low"
                    gemini_config.temperature = 1.0
                else:
                    # Gemini 2.5 ProëŠ” ê¸°ì¡´ ì„¤ì • ìœ ì§€
                    gemini_config.temperature = 0.1

                print(f"[Pipeline] Gemini ëª¨ë¸: {self.config.gemini_model}")
                self._stt_engine = GeminiSTT(gemini_config)
            elif self.config.stt_engine == "whisperx":
                from src.stt.whisperx import WhisperXSTT, WhisperXConfig
                # VRAMì— ë”°ë¥¸ ìë™ ì„¤ì •
                if self.config.vram_gb and self.config.vram_gb <= 6:
                    whisperx_config = WhisperXConfig.for_low_vram(self.config.vram_gb)
                elif self.config.device == "cpu":
                    whisperx_config = WhisperXConfig.for_cpu()
                else:
                    whisperx_config = WhisperXConfig(device=self.config.device)
                self._stt_engine = WhisperXSTT(whisperx_config)
            else:
                raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” STT ì—”ì§„: {self.config.stt_engine}")
        return self._stt_engine

    def _get_timestamp_aligner(self):
        if self._timestamp_aligner is None:
            from src.postprocess.timestamp_aligner import TimestampAligner, TimestampAlignerConfig
            self._timestamp_aligner = TimestampAligner(TimestampAlignerConfig(
                device=self.config.device
            ))
        return self._timestamp_aligner

    def _get_llm_corrector(self):
        if self._llm_corrector is None:
            from src.postprocess.llm_corrector import LLMCorrector
            self._llm_corrector = LLMCorrector()
        return self._llm_corrector

    def _get_speaker_diarizer(self):
        if self._speaker_diarizer is None:
            from src.diarization.speaker import SpeakerDiarizer, SpeakerDiarizerConfig
            self._speaker_diarizer = SpeakerDiarizer(SpeakerDiarizerConfig(
                device=self.config.device
            ))
        return self._speaker_diarizer

    def _get_temp_dir(self) -> Path:
        if self._temp_dir is None:
            if self.config.temp_dir:
                self._temp_dir = Path(self.config.temp_dir)
                self._temp_dir.mkdir(parents=True, exist_ok=True)
            else:
                self._temp_dir = Path(tempfile.mkdtemp(prefix="vtt_pipeline_"))
        return self._temp_dir

    def _cleanup(self) -> None:
        if self.config.cleanup_temp and self._temp_dir and self._temp_dir.exists():
            shutil.rmtree(self._temp_dir, ignore_errors=True)
            self._temp_dir = None

    async def run(
        self,
        input_path: str | Path,
        output_dir: Optional[str | Path] = None,
    ) -> PipelineResult:
        """
        íŒŒì´í”„ë¼ì¸ ì‹¤í–‰

        Args:
            input_path: ì…ë ¥ íŒŒì¼ ê²½ë¡œ
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬

        Returns:
            PipelineResult ê°ì²´
        """
        input_path = Path(input_path)
        if not input_path.exists():
            return PipelineResult(
                success=False,
                error=f"ì…ë ¥ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {input_path}"
            )

        # ğŸ¬ íŒŒì¼ í™•ì¥ì ê¸°ë°˜ ì˜ìƒ/ì˜¤ë””ì˜¤ ìë™ ê°ì§€
        VIDEO_EXTENSIONS = {'.mp4', '.mkv', '.avi', '.mov', '.webm', '.m4v', '.flv'}
        AUDIO_EXTENSIONS = {'.mp3', '.wav', '.m4a', '.flac', '.ogg', '.aac', '.wma'}
        file_ext = input_path.suffix.lower()

        if file_ext in VIDEO_EXTENSIONS:
            # ì˜ìƒ íŒŒì¼ â†’ ì˜ìƒ ëª¨ë“œ ìë™ í™œì„±í™”
            if not self.config.use_video_mode:
                print(f"[Pipeline] ğŸ¬ ì˜ìƒ íŒŒì¼ ê°ì§€ ({file_ext}) â†’ ì˜ìƒ ëª¨ë“œ ìë™ í™œì„±í™”")
                self.config.use_video_mode = True
        elif file_ext in AUDIO_EXTENSIONS:
            # ì˜¤ë””ì˜¤ íŒŒì¼ â†’ ì˜ìƒ ëª¨ë“œ ë¹„í™œì„±í™”
            if self.config.use_video_mode:
                print(f"[Pipeline] ğŸµ ì˜¤ë””ì˜¤ íŒŒì¼ ê°ì§€ ({file_ext}) â†’ ì˜ìƒ ëª¨ë“œ ìë™ ë¹„í™œì„±í™”")
                self.config.use_video_mode = False
        else:
            print(f"[Pipeline] âš ï¸ ì•Œ ìˆ˜ ì—†ëŠ” í™•ì¥ì ({file_ext}) â†’ ê¸°ì¡´ ì„¤ì • ìœ ì§€")

        output_dir = Path(output_dir) if output_dir else input_path.parent
        output_dir.mkdir(parents=True, exist_ok=True)
        base_name = input_path.stem

        processing_info = {
            "mode": self.config.mode.value,
            "stt_engine": self.config.stt_engine,
            "steps_completed": []
        }

        try:
            # 1. ì˜¤ë””ì˜¤ ë¶„ì„
            self._report_progress("ë¶„ì„", 0.05)
            analyzer = self._get_analyzer()
            metadata = analyzer.analyze(input_path)
            processing_info["steps_completed"].append("audio_analysis")

            # 2. ì˜¤ë””ì˜¤ ì¶”ì¶œ
            self._report_progress("ì˜¤ë””ì˜¤ ì¶”ì¶œ", 0.1)
            extractor = self._get_extractor()
            temp_dir = self._get_temp_dir()
            audio_path = temp_dir / f"audio.{self.config.audio_format}"
            extractor.extract(input_path, audio_path)
            processing_info["steps_completed"].append("audio_extraction")

            # 3. ì „ì‚¬ (STT)
            self._report_progress("ì „ì‚¬", 0.3)
            stt_engine = self._get_stt_engine()
            transcription = await stt_engine.transcribe(
                str(audio_path),
                language=self.config.language,
                num_speakers=self.config.num_speakers,
                proper_nouns=self.config.proper_nouns,
                use_video_mode=self.config.use_video_mode,
                original_video_path=str(input_path),  # ì›ë³¸ ì˜ìƒ ê²½ë¡œ ì „ë‹¬
                remove_fillers=self.config.remove_fillers,  # í•„ëŸ¬ ì œê±° ì˜µì…˜
                election_debate_mode=self.config.election_debate_mode  # ì„ ê±° í† ë¡ íšŒ ëª¨ë“œ
            )
            processing_info["steps_completed"].append(f"stt_{self.config.stt_engine}")
            if self.config.use_video_mode:
                processing_info["steps_completed"].append("video_mode_enabled")
            if self.config.remove_fillers:
                processing_info["steps_completed"].append("filler_removal_enabled")
            if self.config.election_debate_mode:
                processing_info["steps_completed"].append("election_debate_mode_enabled")

            # 4. Pyannote í™”ìë¶„ë¦¬ (ì„ íƒì , Gemini í™”ìë¶„ë¦¬ ëŒ€ì²´)
            if self.config.enable_pyannote_diarization:
                self._report_progress("í™”ì ë¶„ë¦¬ (Pyannote)", 0.5)
                try:
                    diarizer = self._get_speaker_diarizer()
                    diarization = await diarizer.diarize(
                        str(audio_path),
                        self.config.num_speakers
                    )
                    transcription = diarizer.align_speakers(transcription, diarization)
                    processing_info["steps_completed"].append("pyannote_diarization")
                except Exception as e:
                    processing_info["pyannote_error"] = str(e)

            # 5. íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬ (Phase 2)
            if self.config.enable_timestamp_alignment:
                self._report_progress("íƒ€ì„ìŠ¤íƒ¬í”„ ì •ë ¬", 0.6)
                try:
                    aligner = self._get_timestamp_aligner()
                    transcription = await aligner.align(str(audio_path), transcription)
                    processing_info["steps_completed"].append("timestamp_alignment")
                except Exception as e:
                    processing_info["alignment_error"] = str(e)

            # 6. LLM êµì • (Phase 2)
            if self.config.enable_llm_correction:
                self._report_progress("LLM êµì •", 0.75)
                try:
                    corrector = self._get_llm_corrector()
                    # ê³ ìœ ëª…ì‚¬ íŒíŠ¸ë¥¼ context_hintë¡œ ì „ë‹¬
                    context_hint = None
                    if self.config.proper_nouns:
                        context_hint = f"ë“±ì¥ ì¸ë¬¼/ìš©ì–´: {', '.join(self.config.proper_nouns)}"
                    transcription = await corrector.correct(transcription, context_hint=context_hint)
                    processing_info["steps_completed"].append("llm_correction")
                except Exception as e:
                    processing_info["correction_error"] = str(e)

            # 7. ì„¸ê·¸ë¨¼íŠ¸ ë³‘í•©
            self._report_progress("í›„ì²˜ë¦¬", 0.85)
            transcription = transcription.merge_consecutive_segments()
            processing_info["steps_completed"].append("merge_segments")

            # 8. ì¶œë ¥ íŒŒì¼ ìƒì„±
            self._report_progress("ì¶œë ¥ ìƒì„±", 0.9)
            output_files = {}

            for fmt in self.config.output_formats:
                output_path = output_dir / f"{base_name}.{fmt.value}"

                if fmt == OutputFormat.SRT:
                    generator = SRTGenerator(SRTConfig(
                        include_speaker=self.config.include_speaker_labels
                    ))
                    generator.save(transcription, output_path)

                elif fmt == OutputFormat.VTT:
                    generator = VTTGenerator(SRTConfig(
                        include_speaker=self.config.include_speaker_labels
                    ))
                    generator.save(transcription, output_path)

                elif fmt == OutputFormat.JSON:
                    exporter = JSONExporter(JSONExportConfig())
                    exporter.save(transcription, output_path, str(input_path))

                elif fmt == OutputFormat.TXT:
                    with open(output_path, "w", encoding="utf-8") as f:
                        f.write(transcription.full_text)

                output_files[fmt.value] = output_path

            self._report_progress("ì™„ë£Œ", 1.0)

            return PipelineResult(
                success=True,
                transcription=transcription,
                output_files=output_files,
                audio_metadata=metadata,
                processing_info=processing_info
            )

        except Exception as e:
            return PipelineResult(
                success=False,
                error=str(e),
                processing_info=processing_info
            )

        finally:
            self._cleanup()

    def run_sync(
        self,
        input_path: str | Path,
        output_dir: Optional[str | Path] = None,
    ) -> PipelineResult:
        """ë™ê¸° ì‹¤í–‰ ë˜í¼"""
        return asyncio.run(self.run(input_path, output_dir))


def create_pipeline(
    mode: str = "fast",
    language: str = "ko",
    num_speakers: Optional[int] = None,
    output_formats: Optional[list[str]] = None,
    include_speaker_labels: bool = True,
    device: str = "cuda",
) -> TranscriptionPipeline:
    """
    íŒŒì´í”„ë¼ì¸ íŒ©í† ë¦¬ í•¨ìˆ˜

    Args:
        mode: íŒŒì´í”„ë¼ì¸ ëª¨ë“œ ("fast", "accurate", "hybrid", "full")
        language: ì–¸ì–´ ì½”ë“œ
        num_speakers: í™”ì ìˆ˜ íŒíŠ¸
        output_formats: ì¶œë ¥ í¬ë§· ë¦¬ìŠ¤íŠ¸ (["srt", "json"])
        include_speaker_labels: í™”ì ë ˆì´ë¸” í¬í•¨ ì—¬ë¶€
        device: ì—°ì‚° ì¥ì¹˜ ("cuda" ë˜ëŠ” "cpu")

    Returns:
        ì„¤ì •ëœ TranscriptionPipeline
    """
    # ëª¨ë“œ íŒŒì‹±
    try:
        pipeline_mode = PipelineMode(mode.lower())
    except ValueError:
        pipeline_mode = PipelineMode.FAST

    # ì¶œë ¥ í¬ë§· íŒŒì‹±
    formats = []
    if output_formats:
        for fmt in output_formats:
            try:
                formats.append(OutputFormat(fmt.lower()))
            except ValueError:
                pass
    if not formats:
        formats = [OutputFormat.SRT]

    config = PipelineConfig(
        mode=pipeline_mode,
        language=language,
        num_speakers=num_speakers,
        output_formats=formats,
        include_speaker_labels=include_speaker_labels,
        device=device
    )

    return TranscriptionPipeline(config)


# í¸ì˜ í•¨ìˆ˜ë“¤
def create_fast_pipeline(**kwargs) -> TranscriptionPipeline:
    """ë¹ ë¥¸ íŒŒì´í”„ë¼ì¸ (Geminië§Œ)"""
    return create_pipeline(mode="fast", **kwargs)


def create_accurate_pipeline(**kwargs) -> TranscriptionPipeline:
    """ì •í™•í•œ íŒŒì´í”„ë¼ì¸ (WhisperX)"""
    return create_pipeline(mode="accurate", **kwargs)


def create_hybrid_pipeline(**kwargs) -> TranscriptionPipeline:
    """í•˜ì´ë¸Œë¦¬ë“œ íŒŒì´í”„ë¼ì¸ (Gemini + WhisperX ì •ë ¬)"""
    return create_pipeline(mode="hybrid", **kwargs)


def create_full_pipeline(**kwargs) -> TranscriptionPipeline:
    """ì „ì²´ íŒŒì´í”„ë¼ì¸ (í•˜ì´ë¸Œë¦¬ë“œ + LLM êµì •)"""
    return create_pipeline(mode="full", **kwargs)
